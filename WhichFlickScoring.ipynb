{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from math import log\n",
    "import heapq\n",
    "import json\n",
    "import requests\n",
    "from nltk.corpus import wordnet as wn\n",
    "import os\n",
    "import gensim \n",
    "from gensim.models import Word2Vec\n",
    "nltk.download('punkt')\n",
    "import sqlite3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the sentence list from the db for the embedded model\n",
    "os.chdir('/media/removable/sdcard/CompTools/computational-tools-2018')\n",
    "\n",
    "conn = sqlite3.connect('./backend/parser.sqlite3')\n",
    "c = conn.cursor()\n",
    "sentences=[]\n",
    "x=c.execute('Select title,tokenized_plot From movies where tokenized_plot is not NULL;')\n",
    "for movie in x:\n",
    "    print((movie[1]))\n",
    "    sentences.append(movie[1].split(\",\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create CBOW model\n",
    "model1 = gensim.models.Word2Vec(sentences, min_count = 1,  \n",
    "                              size = 100, window = 5) \n",
    "\n",
    "#create skip-grams model\n",
    "model2 = gensim.models.Word2Vec(sentences, min_count = 1, size = 100, \n",
    "                                             window = 5, sg = 1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the models to files\n",
    "os.chdir(\"/media/removable/sdcard/CompTools\")\n",
    "model1.wv.save_word2vec_format('model1.bin')\n",
    " \n",
    "model2.wv.save_word2vec_format('model2.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating synonyms from inputs    \n",
    "\n",
    "inputArgs = sys.argv\n",
    "synonymList=[list() for arg in sys.argv]\n",
    "\n",
    "#grab synonyms from API\n",
    "for position in range(len(sys.argv)):\n",
    "    r = requests.get('https://od-api.oxforddictionaries.com:443/api/v1/entries/en/' + synonymList[position]+ '/synonyms', headers = {'app_id': 'a76c92b5', 'app_key': '107c3736bfdc71a084306ecb73aafa26'})\n",
    "    synonyms=json.loads(r.text)\n",
    "    for i in synonyms[\"results\"]:\n",
    "        for j in i[\"lexicalEntries\"]:\n",
    "            for k in j[\"entries\"]:\n",
    "                for v in k[\"senses\"]:\n",
    "                    for w in v[\"synonyms\"]:\n",
    "                        synonymList[position].append(w[\"text\"])\n",
    "\n",
    "#load the desired model\n",
    "model1 = Word2Vec.load('model1.bin')\n",
    "\n",
    "#find synonyms from embedded model\n",
    "for i in range(len(synonymList)):\n",
    "    for word in word2vec.wv.most_similar(inputArgs[i]):\n",
    "        synonymList[i].append(word[0])\n",
    "\n",
    "#eliminate any duplicate synonyms\n",
    "for each in synonymList:\n",
    "    each=set([word for word in each if len(word.split()) == 1])\n",
    "\n",
    "    \n",
    "#-------------datamuse api--------------------------------------------------------------------    \n",
    "# for i in range(len(synonymList)) in synonymList:\n",
    "#     response=requests.get(\"https://api.datamuse.com/words?ml=\",inputArgs[i],\"&max=5\")\n",
    "#     first_syns=json.loads(response.text)\n",
    "#     for first_syn in first_syns:\n",
    "#          second_syns=wordnet.synsets(first_syn)\n",
    "#             for second_syn in second_syns:\n",
    "#                 third_syns=pyhugeconnector.thesaurus_entry(\n",
    "#                 word=item, \n",
    "#                 api_key='e2e7d8c39bbda388d13c8dbcbb25533b', \n",
    "#                 pos_tag='n', \n",
    "#                 ngram=1, \n",
    "#                 relationship_type='syn') \n",
    "#                 for third_syn in third_syns:\n",
    "#                     synonymList[i].append(third_syn)\n",
    "#     synonymList[i]=set(synonymList[i])\n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "#--------------pyhugeconnector api------------------------------------------------------------\n",
    "\n",
    "# for word_dict in jsontext:\n",
    "#     synlist.append(word_dict['word'])\n",
    "\n",
    "# for item in range(len(inputArgs)):\n",
    "#     synonymList.append(sys.argv[item+1])\n",
    "#     nltkSyns = wordnet.synsets(sys.argv[item+1]) \n",
    "#     for firstSyn in nltkSyns:\n",
    "#         wordSynonyms=pyhugeconnector.thesaurus_entry(\n",
    "#             word=item, \n",
    "#             api_key='e2e7d8c39bbda388d13c8dbcbb25533b', \n",
    "#             pos_tag='n', \n",
    "#             ngram=1, \n",
    "#             relationship_type='syn') \n",
    "        #Use ‘syn’ for synonyms, ‘ant’ for antonyms, ‘rel’ for related terms, ‘sim’ \n",
    "        #for similar terms,‘usr’ for user suggestions and None for all (default: None)\n",
    "        \n",
    "#         for secondSyn in wordSynonyms:\n",
    "#             synonymList[item].append(secondSyn)\n",
    "#     synonymList[item]=set(synonymList[item])\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#use the bloom filter here\n",
    "\n",
    "\n",
    "#grab db entries for synonym info\n",
    "flat_synonyms=','.join(item for sublist in synonymList for item in sublist)\n",
    "\n",
    "db=sqlite3.connect('./backend/parser.sqlite3')\n",
    "cursor=db.cursor()\n",
    "sqlCall=db.execute(\"SELECT word,movie_ids from word_movies where word in (\"+flat_synonyms+\")\") \n",
    "\n",
    "#make dictionary from the sql call\n",
    "wordDict={}\n",
    "for entry in sqlCall:\n",
    "    wordDict[entry[0]]=entry[1].split(\",\")\n",
    "\n",
    "#loop through the dictionary and sum the movies\n",
    "movieScores={}\n",
    "for group in range(len(synonymList)):\n",
    "    for word in synonymList[group]:\n",
    "        if movieScores.get(movie):\n",
    "            movieScores[word][group]+=1\n",
    "        else:\n",
    "            movieScores[word]=[0,0,0]\n",
    "            movieScores[word][group]=1    \n",
    "    \n",
    "for movie in movieScores:\n",
    "    original_syn_count=0\n",
    "    for word in inputArgs:\n",
    "        if movie in wordDict[word]:\n",
    "            original_syn_count+=1\n",
    "    resultDictionary[movie]=math.log(movieScores[movie][0]+0.00001, 1.5)+math.log(movieScores[movie][1]+0.00001, 1.5)+math.log(movieScores[movie][2]+0.00001, 1.5)+5*original_syn_count\n",
    "    \n",
    "recommendation=heapq.nlargest(10,resultDictionary.values())\n",
    "print(\"Based on your inputs, we recommend the following:\",'\\n')\n",
    "for movie in recommendation:\n",
    "    print(movie)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
