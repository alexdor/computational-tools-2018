{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'stopwords'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-d25a6c503ef0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwordnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyhugeconnector\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyhugeconnector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mheapq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'stopwords'"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize,stopwords,wordnet\n",
    "import nltk\n",
    "from pyhugeconnector import pyhugeconnector\n",
    "from math import log\n",
    "import heapq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the plot\n",
    "#tokenize character profile simpsons\n",
    "\n",
    "    \n",
    "# db = sqlite3.connect(\"movies\")\n",
    "# cursor=db.cursor()\n",
    "# sqlCall = db.execute(\"SELECT title,plot from movies\")\n",
    "\n",
    "# for movie in sqlCall:\n",
    "#     tokens=word_tokenize(movie[1])\n",
    "#     cursor.execute('''UPDATE movies SET tokenizedPlot = ? WHERE (title = ?)''',(tokens, movie[0]))\n",
    "# db.commit()\n",
    "# db.close()\n",
    "\n",
    "#tokenize quotes\n",
    "db=sqlite3.connect(\"movies\")\n",
    "stop = stopwords.words(\"english\")\n",
    "cursor=db.cursor()\n",
    "sqlCall=db.execute(\"SELECT title,plot from movies\")\n",
    "\n",
    "for movie in sqlCall:\n",
    "    text=movie[1].lower()\n",
    "    text=re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    text = \" \".join([word for word in text.split() if word not in stop])\n",
    "    text=re.sub(r\"rt\\s\", \"\", text)\n",
    "    text=re.sub(r\"rt\\t\", \"\", text)\n",
    "    text=re.sub(r\"\\d+\", \"\", text)\n",
    "    cursor.execute(\"UPDATE movies SET tokenizedPlot = ? WHERE (title=?)\",(text,movie[0]))\n",
    "db.commit()\n",
    "db.close()\n",
    "\n",
    "\n",
    "#create dictionary of words\n",
    "wordDictionary={}\n",
    "\n",
    "sqlCall=db.execute(\"SELECT pageID,tokenizedPlot from movies\")\n",
    "for movie in sqlCall:\n",
    "    for word in set(movie[1]):\n",
    "        wordDictionary[word].append(movie[0])\n",
    "\n",
    "\n",
    "# creating synonyms from inputs    \n",
    "inputArgs = sys.argv\n",
    "synonymList=[[],[],[]]\n",
    "for item in range(len(inputArgs)):\n",
    "    synonymList.append(sys.argv[item+1])\n",
    "    nltkSyns = wordnet.synsets(sys.argv[item+1]) \n",
    "    for firstSyn in nltkSyns:\n",
    "        wordSynonyms=pyhugeconnector.thesaurus_entry(\n",
    "            word=item, \n",
    "            api_key='e2e7d8c39bbda388d13c8dbcbb25533b', \n",
    "            pos_tag='n', \n",
    "            ngram=1, \n",
    "            relationship_type='syn') \n",
    "        #Use ‘syn’ for synonyms, ‘ant’ for antonyms, ‘rel’ for related terms, ‘sim’ \n",
    "        #for similar terms,‘usr’ for user suggestions and None for all (default: None)\n",
    "        \n",
    "        for secondSyn in wordSynonyms:\n",
    "            synonymList[item].append(secondSyn)\n",
    "    synonymList[item]=set(synonymList[item])\n",
    "\n",
    "\n",
    "#use the bloom filter to get rid of nonsense words\n",
    "\n",
    "#gather movies based on the wordDictionary\n",
    "resultDictionary={}\n",
    "checkDictionary={}\n",
    "for group in range(len(synonymList)):\n",
    "    for word in group:\n",
    "        for movie in wordDictionary[word]:\n",
    "            resultDictionary[movie].append(group)\n",
    "            checkDictionary[movie].append(word)\n",
    "for movie in resultDictionary:\n",
    "    resultDictionary[movie]=math.log(count(0), 1.5)+math.log(count(1), 1.5)+math.log(count(2), 1.5)+5*len(set(resultDictionary[movie]))\n",
    "    \n",
    "recommendation=heapq.nlargest(10,resultDictionary.values())\n",
    "print(\"Based on your inputs, we recommend the following:\",'\\n')\n",
    "for movie in recommendation:\n",
    "    print(movie)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/andy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "['defeat', 'pop', 'down', 'obliterate', 'ache', 'be', 'beat', 'blackball', 'cut', 'destroy', 'destruct', 'drink', 'end', 'exhaust', 'hit', 'hurt', 'imbibe', 'negative', 'overcome', 'overpower', 'overtake', 'overwhelm', 'suffer', 'terminate', 'tucker', 'veto', 'whelm']\n"
     ]
    }
   ],
   "source": [
    " nltk.download('wordnet')\n",
    "wordSynonyms=pyhugeconnector.thesaurus_entry(\n",
    "        word='kill', \n",
    "        api_key='e2e7d8c39bbda388d13c8dbcbb25533b', \n",
    "        pos_tag=nltk.corpus.wordnet.VERB, \n",
    "        ngram=1, \n",
    "        relationship_type='syn')\n",
    "print(wordSynonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-76a01d9c502b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
